{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <center>  Project 4  Web Scraping + NLP </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucky/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Project, We will explore trending posts and comments on reddit. Reddit is a community-driven link-sharing site. Users submit links to articles, photos, and other content. Other users upvote the submissions they like, and downvote the ones they dislike. Users can comment on submissions, and even upvote or downvote other people's comments. Reddit consists of many smaller communities called subreddits where more focused communities can discuss niche posts. we will start by gathering the data using tools like webdriver, and with the help of BeautifulSoup we will read and save that data into lists.\n",
    "We will start our initial analysis about the data by making one DataFrame from those lists, We will check for any missing values, If present we will be replacing it or getting rid of that particular row depending on quality and quantity of missing values. Next, We will convert the columns to numerical columns, here librarires HotEncoder, or labelEncoder will be used. Once we have our DataFrame ready to go, we will do Our Analysis. We will convert the Number of Comment columns (which is our y in this case) to binary column, we will check for the baseline. Then, We will create two subsets of our dataframes, One will contain values with the number of comments above the median and the other dataset will contain the values with the number of comments below the median. And we will check The Title name columns of those datasets, we will check the most occurring words, and try to understand what make one post to be popular than the other. Once, we have the better understanding of the words in the each sub-DataFrame We will use Sentiment Analysis library, like Vader, which can be used to identify the most positive and negative words in the DataFrame. Next, We will try to create a model which can predict by analyzing the name of the title if the particular post will have number of comments below median or above median. Here we will test different classifier models, like DecisionTreeClassifier, RandomForestClassifier, LogisticRegression etc. we will further try to improve our accuracy score with other gradientboosting algorithm such as: adaboost, Xgboost or GradientBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *__Web Scraping: __*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "limit = 0\n",
    "page = 'http://www.reddit.com'\n",
    "\n",
    "#Below Lists will store our scraped data\n",
    "titles = []\n",
    "time_since = []\n",
    "comments = []\n",
    "subreddit = []\n",
    "\n",
    "# j will be used in our function below to get rid of 25th element (Advertised title)\n",
    "j = 0\n",
    "\n",
    "#will keep track of loops\n",
    "limit = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "while limit <= 200:\n",
    "    \n",
    "   #------------------------------------------------------------------------# \n",
    "    driver = webdriver.Chrome(executable_path=\"./chromedriver/chromedriver\")\n",
    "    driver.get(page)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    sleep(10)\n",
    "    \n",
    "   # Above we are setting our webdriver, which is getting data from Reddit,\n",
    "   # as html and converts it into lxml format for BeautfulSoup which we will\n",
    "   # use below to extract data.\n",
    "   #------------------------------------------------------------------------#\n",
    "    \n",
    "    \n",
    "    # Below are scraping each Title from the page.\n",
    "    for title in soup.find_all('p', {'class' : 'title' }):\n",
    "        try:\n",
    "            titles.append(title.text)\n",
    "        \n",
    "        except:\n",
    "            print \"Iteration :\", limit, \"Unsuccessfull attempt to Extract Title, BREAK!!!\"\n",
    "            titles.append('Error')\n",
    "            break\n",
    "    sleep(2)\n",
    "    \n",
    "    # Extracting Live-Timestamp and saving it in a list.\n",
    "    for time in soup.find_all('time', {'class' : 'live-timestamp'}):\n",
    "        try:\n",
    "            time_since.append(time.text)\n",
    "       \n",
    "        except:\n",
    "            print \"Iteration :\", limit, \"Unsuccessfull attempt to Extract Time_since, BREAK!!!\"\n",
    "            time_since.append('Error')\n",
    "            break\n",
    "            \n",
    "    sleep(2)\n",
    "    \n",
    "    # Extracing subreddit and saving in into a list.\n",
    "    for x in soup.find_all('a', {'class' : 'subreddit hover may-blank'}):\n",
    "        try:\n",
    "            subreddit.append(x.text[2:])\n",
    "    \n",
    "        except:\n",
    "            print \"Iteration :\", limit, \"Unsuccessfull attempt to Extract Subreddit, BREAK!!!!\"\n",
    "            subreddit.append('Error')\n",
    "            break\n",
    "    \n",
    "    \n",
    "    sleep(2)\n",
    "    \n",
    "    # Scraping Comments\n",
    "    for i in soup.find_all('li', {'class' : 'first'}):\n",
    "        try:\n",
    "            comments.append(i.text.split()[0])\n",
    "        except:\n",
    "            print \"Iteration :\", limit, \"Unsuccessfull attempt to Extract Comment, BREAK!!!!\"\n",
    "            comments.append('Error')\n",
    "            break\n",
    "    \n",
    "    # Finds next page and clicks on it and save the link into page.\n",
    "    try:\n",
    "        page = soup.find('span', {'class':'next-button'}).a['href']\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    #-------------#\n",
    "    del titles[j]  # Get rids of top title from each page, which is advertised title.\n",
    "    j += 25\n",
    "    #-------------#\n",
    "    \n",
    "    print(\"Length of List TITLES: \", len(titles))\n",
    "    print(\"Length of Comments: \", len(comments)) #Printing length of each list to check \n",
    "    print(\"Length of Subreddit: \", len(subreddit))\n",
    "    print(\"Length of time_since: \", len(time_since))\n",
    "    \n",
    "    driver.close()\n",
    "    limit += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we are a dataframe with the data we scraped above and saving it as test.csv into project folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "test = pd.DataFrame()\n",
    "test['Comments'] = comments\n",
    "test['Time_Since'] = time_since\n",
    "test['Subreddit'] = subreddit\n",
    "test['Titles'] = titles\n",
    "test.to_csv('test.csv', encoding = 'utf-8')\n",
    "\n",
    "#------------------------------Done with Web Scraping -------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>time_delta</th>\n",
       "      <th>time_now</th>\n",
       "      <th>title</th>\n",
       "      <th>upvotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-22 16:36:35</td>\n",
       "      <td>470616764</td>\n",
       "      <td>1153</td>\n",
       "      <td>gifs</td>\n",
       "      <td>0 days 04:24:57.883078000</td>\n",
       "      <td>2018-01-22 21:01:32.883070</td>\n",
       "      <td>Finnish ski jumping team</td>\n",
       "      <td>86005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-22 16:37:28</td>\n",
       "      <td>470617063</td>\n",
       "      <td>238</td>\n",
       "      <td>pics</td>\n",
       "      <td>0 days 04:24:04.883093000</td>\n",
       "      <td>2018-01-22 21:01:32.883092</td>\n",
       "      <td>Super excited about motherhood</td>\n",
       "      <td>20336</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0           created_at         id  num_comments subreddit  \\\n",
       "0           0  2018-01-22 16:36:35  470616764          1153      gifs   \n",
       "1           1  2018-01-22 16:37:28  470617063           238      pics   \n",
       "\n",
       "                  time_delta                    time_now  \\\n",
       "0  0 days 04:24:57.883078000  2018-01-22 21:01:32.883070   \n",
       "1  0 days 04:24:04.883093000  2018-01-22 21:01:32.883092   \n",
       "\n",
       "                            title  upvotes  \n",
       "0        Finnish ski jumping team    86005  \n",
       "1  Super excited about motherhood    20336  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5567, 9)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print data.shape\n",
    "print data.isnull().any().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Series.astype of 0       2018-01-22 16:36:35\n",
       "1       2018-01-22 16:37:28\n",
       "2       2018-01-22 16:50:12\n",
       "3       2018-01-22 17:25:20\n",
       "4       2018-01-22 15:41:56\n",
       "5       2018-01-22 15:01:59\n",
       "6       2018-01-22 15:35:00\n",
       "7       2018-01-22 14:51:38\n",
       "8       2018-01-22 15:08:03\n",
       "9       2018-01-22 17:38:32\n",
       "10      2018-01-22 15:14:57\n",
       "11      2018-01-22 16:49:29\n",
       "12      2018-01-22 15:25:06\n",
       "13      2018-01-22 14:16:41\n",
       "14      2018-01-22 14:17:39\n",
       "15      2018-01-22 14:10:24\n",
       "16      2018-01-22 15:38:09\n",
       "17      2018-01-22 14:46:49\n",
       "18      2018-01-22 13:55:39\n",
       "19      2018-01-22 14:16:05\n",
       "20      2018-01-22 15:39:41\n",
       "21      2018-01-22 13:15:12\n",
       "22      2018-01-22 13:52:23\n",
       "23      2018-01-22 15:10:47\n",
       "24      2018-01-22 13:13:34\n",
       "25      2018-01-22 14:24:57\n",
       "26      2018-01-22 16:39:12\n",
       "27      2018-01-22 14:54:27\n",
       "28      2018-01-22 14:25:11\n",
       "29      2018-01-22 18:00:48\n",
       "               ...         \n",
       "5537    2018-01-22 17:50:01\n",
       "5538    2018-01-22 19:15:30\n",
       "5539    2018-01-22 11:55:41\n",
       "5540    2018-01-22 19:55:51\n",
       "5541    2018-01-22 18:07:11\n",
       "5542    2018-01-22 16:17:18\n",
       "5543    2018-01-22 18:07:01\n",
       "5544    2018-01-22 03:33:07\n",
       "5545    2018-01-22 20:22:07\n",
       "5546    2018-01-21 23:05:11\n",
       "5547    2018-01-22 02:41:13\n",
       "5548    2018-01-22 19:33:32\n",
       "5549    2018-01-22 18:59:07\n",
       "5550    2018-01-22 10:13:12\n",
       "5551    2018-01-22 17:36:54\n",
       "5552    2018-01-22 18:30:21\n",
       "5553    2018-01-22 20:29:39\n",
       "5554    2018-01-22 07:01:06\n",
       "5555    2018-01-22 13:27:09\n",
       "5556    2018-01-22 18:55:40\n",
       "5557    2018-01-22 19:34:12\n",
       "5558    2018-01-22 02:43:15\n",
       "5559    2018-01-22 15:47:29\n",
       "5560    2018-01-22 18:36:10\n",
       "5561    2018-01-22 09:59:31\n",
       "5562    2018-01-22 00:47:45\n",
       "5563    2018-01-22 11:43:07\n",
       "5564    2018-01-22 17:50:37\n",
       "5565    2018-01-21 21:20:19\n",
       "5566    2018-01-22 17:21:55\n",
       "Name: created_at, Length: 5567, dtype: object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.created_at.astype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are only going to keep Commments, Time_since, Subreddit, Title and upvotes.\n",
    "Few columns need little cleaning, Below we are going over each columns that needs cleaning, we are going manipulate it, to obtain maximum usage of that columns data.\n",
    "-We are not going to include creation time and time_now, as time_delta represents the difference between those two, So no need to add those two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Comments'] = data['num_comments']\n",
    "df['Time_Since'] = data['time_delta'] # Represents time difference between creation and present(scraped time).\n",
    "df['Subreddit'] = data['subreddit']\n",
    "df['Titles'] = data['title']\n",
    "df['upvotes'] = data['upvotes']\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our Time_since column we need to keep only first two integers, and get rid of string values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting time_Since to just int val hours.\n",
    "def my_func(obj):\n",
    "    obj = obj[7:9]\n",
    "    obj = obj\n",
    "    return obj\n",
    "\n",
    "df.Time_Since = df.Time_Since.apply(my_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comments</th>\n",
       "      <th>Time_Since</th>\n",
       "      <th>Subreddit</th>\n",
       "      <th>Titles</th>\n",
       "      <th>upvotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1153</td>\n",
       "      <td>04</td>\n",
       "      <td>gifs</td>\n",
       "      <td>Finnish ski jumping team</td>\n",
       "      <td>86005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>238</td>\n",
       "      <td>04</td>\n",
       "      <td>pics</td>\n",
       "      <td>Super excited about motherhood</td>\n",
       "      <td>20336</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Comments Time_Since Subreddit                          Titles  upvotes\n",
       "0      1153         04      gifs        Finnish ski jumping team    86005\n",
       "1       238         04      pics  Super excited about motherhood    20336"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = preprocessing.LabelEncoder()\n",
    "df.Subreddit = encoder.fit_transform(df.Subreddit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comments</th>\n",
       "      <th>Time_Since</th>\n",
       "      <th>Subreddit</th>\n",
       "      <th>Titles</th>\n",
       "      <th>upvotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1153</td>\n",
       "      <td>04</td>\n",
       "      <td>1144</td>\n",
       "      <td>Finnish ski jumping team</td>\n",
       "      <td>86005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>238</td>\n",
       "      <td>04</td>\n",
       "      <td>1409</td>\n",
       "      <td>Super excited about motherhood</td>\n",
       "      <td>20336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>844</td>\n",
       "      <td>04</td>\n",
       "      <td>1112</td>\n",
       "      <td>Messing with the new guy.</td>\n",
       "      <td>17611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>380</td>\n",
       "      <td>03</td>\n",
       "      <td>1531</td>\n",
       "      <td>NASA cancels and postpones all of their public...</td>\n",
       "      <td>11178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>430</td>\n",
       "      <td>05</td>\n",
       "      <td>1579</td>\n",
       "      <td>New Bill Would Stop States From Banning Broadb...</td>\n",
       "      <td>13467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Comments Time_Since  Subreddit  \\\n",
       "0      1153         04       1144   \n",
       "1       238         04       1409   \n",
       "2       844         04       1112   \n",
       "3       380         03       1531   \n",
       "4       430         05       1579   \n",
       "\n",
       "                                              Titles  upvotes  \n",
       "0                           Finnish ski jumping team    86005  \n",
       "1                     Super excited about motherhood    20336  \n",
       "2                          Messing with the new guy.    17611  \n",
       "3  NASA cancels and postpones all of their public...    11178  \n",
       "4  New Bill Would Stop States From Banning Broadb...    13467  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "median_comment = df.Comments.median()\n",
    "def to_num(comments):\n",
    "    if comments > median_comment:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df.Comments = df.Comments.apply(to_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmAAAAHgCAYAAAACM9GVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJGTMOKxJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1117b5250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get plot of Value Counts\n",
    "df.Comments.value_counts().plot(kind='bar', figsize=(10,8))\n",
    "plt.xlabel('0 below Average and 1 Above Average');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comments</th>\n",
       "      <th>Time_Since</th>\n",
       "      <th>Subreddit</th>\n",
       "      <th>Titles</th>\n",
       "      <th>upvotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>04</td>\n",
       "      <td>1144</td>\n",
       "      <td>Finnish ski jumping team</td>\n",
       "      <td>86005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>04</td>\n",
       "      <td>1409</td>\n",
       "      <td>Super excited about motherhood</td>\n",
       "      <td>20336</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Comments Time_Since  Subreddit                          Titles  upvotes\n",
       "0         1         04       1144        Finnish ski jumping team    86005\n",
       "1         1         04       1409  Super excited about motherhood    20336"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "below_median = df[df['Comments'] == 0]\n",
    "above_median = df[df['Comments'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2876, 5)\n",
      "(2691, 5)\n"
     ]
    }
   ],
   "source": [
    "print below_median.shape\n",
    "print above_median.shape  #Almost 50%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>  Most Occurred words in </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>_Posts with Number of Comments above Median <br> vs. <br> Number of Comments Below Median.</center>_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use the TfidfVectorizer to find ngrams for us\n",
    "vect = TfidfVectorizer(ngram_range=(2,3), stop_words='english', lowercase= True)\n",
    "\n",
    "summaries = \"\".join(below_median['Titles'])\n",
    "ngrams_summaries = vect.build_analyzer()(summaries)\n",
    "new_df = pd.DataFrame(Counter(ngrams_summaries).most_common(10), columns = ['Top_Words', 'Times_Occurred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Top_Words</th>\n",
       "      <th>Times_Occurred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fuck sony</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fuck sony fuck</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sony fuck sony</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sony fuck</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>year old</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>super bowl</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>alexis sanchez</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>women march</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ball fighterz</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>years ago</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Top_Words  Times_Occurred\n",
       "0       fuck sony              10\n",
       "1  fuck sony fuck               8\n",
       "2  sony fuck sony               8\n",
       "3       sony fuck               8\n",
       "4        year old               7\n",
       "5      super bowl               7\n",
       "6  alexis sanchez               6\n",
       "7     women march               5\n",
       "8   ball fighterz               5\n",
       "9       years ago               5"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we list shows Most occurred words in Titles which has number of comments below the Median. On any website with user generated content, trolls will appear. The truth about the internet is that offensive languages are unavoidable. Its looks like users are better off ignoring the posts whose title contain one of those words showed in the lists above. Users see these as trolls and will just ignore these kind of posts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'years ago', 13),\n",
       " (u'super bowl', 13),\n",
       " (u'year old', 9),\n",
       " (u'alexis sanchez', 8),\n",
       " (u'looks like', 7),\n",
       " (u'government shutdown', 6),\n",
       " (u'supreme court', 6),\n",
       " (u'oh boy', 6),\n",
       " (u'ago today', 6),\n",
       " (u'years ago today', 5)]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(2,4), stop_words='english')\n",
    "\n",
    "summaries = \"\".join(above_median['Titles'])\n",
    "ngrams_summaries = vect.build_analyzer()(summaries)\n",
    "\n",
    "Counter(ngrams_summaries).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like Posts with neutral or Positive words in title, attracted more Users and encouraged them to comment more, which results in more number of comments, On the other hand as seen above Posts with very offensive language had number of comments below median."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Below we will find the top most negative and Positve features by VADER score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['vader_neg'] = 0\n",
    "df['vader_pos'] = 0\n",
    "df['vader_neu'] = 0\n",
    "df['vader_compound'] = 0\n",
    "for i, q in enumerate(df.Titles.values):\n",
    "    vs = analyzer.polarity_scores(q)\n",
    "    df.iloc[i, -4] = vs['neg']\n",
    "    df.iloc[i, -3] = vs['pos']\n",
    "    df.iloc[i, -2] = vs['neu']\n",
    "    df.iloc[i, -1] = vs['compound']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o shit\n",
      "stolen\n",
      "Sneaky\n",
      "Fail\n",
      "RIOT\n",
      "Shit\n",
      "Evil\n",
      "Damn.\n",
      "The Death Penalty\n",
      "real shit :(\n"
     ]
    }
   ],
   "source": [
    "df.sort_values('vader_neg', ascending=False, inplace=True)\n",
    "for i in range(10):\n",
    "    print df.Titles.values[i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Positive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delicious.\n",
      "Beauty\n",
      "Congratulations!\n",
      "True\n",
      "Joke.\n",
      "Lol\n",
      "Well well\n",
      "Hooray!\n",
      "C L E M Approves\n",
      "lol\n"
     ]
    }
   ],
   "source": [
    "df.sort_values('vader_pos', ascending = False, inplace=True)\n",
    "for i in range(10):\n",
    "    print df.Titles.values[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did our intial analysis, we pretty much got the idia of what do all the titles with number of comments above the average have in common. Now lets just check the baseline, and we will try to create a model intelligent enought to predict if given titles name, to predict if that specific title will have number of comments below or Above average. We will test multiple modelsl, and we will choose one with them most accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51661577151068794"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(df.Comments.value_counts(normalize=True)) # Baseline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets check our models performance solely by sentiment features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.52150538  0.52603232  0.52064632  0.52962298  0.51705566  0.51705566\n",
      "  0.50539568  0.51258993  0.57374101  0.51618705]\n",
      "0.52398319718\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = df[['vader_neg','vader_pos','vader_neu','vader_compound']]\n",
    "y = df.Comments\n",
    "\n",
    "Xs = preprocessing.StandardScaler().fit_transform(X)\n",
    "cv = StratifiedKFold(n_splits=10, random_state=42)  \n",
    "scores = cross_val_score(LogisticRegression(), Xs, y, cv=cv)\n",
    "print scores\n",
    "print np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Barely above the baseline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfv = TfidfVectorizer(ngram_range=(2,4),max_features=7000, lowercase = True, stop_words = 'english', )\n",
    "tfv_matrix = tfv.fit_transform(df.Titles).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5567, 7000)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfv_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title_words = pd.DataFrame(tfv_matrix, columns = tfv.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat([df, title_words], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5567, 7009)\n"
     ]
    }
   ],
   "source": [
    "print df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.drop('Titles', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comments</th>\n",
       "      <th>Time_Since</th>\n",
       "      <th>Subreddit</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>vader_neg</th>\n",
       "      <th>vader_pos</th>\n",
       "      <th>vader_neu</th>\n",
       "      <th>vader_compound</th>\n",
       "      <th>00 30</th>\n",
       "      <th>00 gmt</th>\n",
       "      <th>...</th>\n",
       "      <th>york evidence support</th>\n",
       "      <th>york evidence support theory</th>\n",
       "      <th>yosemite national</th>\n",
       "      <th>yosemite national park</th>\n",
       "      <th>yosemite national park oc</th>\n",
       "      <th>youtube channel</th>\n",
       "      <th>youtube video</th>\n",
       "      <th>zelda cube</th>\n",
       "      <th>zoe van</th>\n",
       "      <th>zoe van dijk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>04</td>\n",
       "      <td>1144</td>\n",
       "      <td>86005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>04</td>\n",
       "      <td>1409</td>\n",
       "      <td>20336</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.743</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 7008 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Comments Time_Since  Subreddit  upvotes  vader_neg  vader_pos  vader_neu  \\\n",
       "0         1         04       1144    86005        0.0      0.000      1.000   \n",
       "1         1         04       1409    20336        0.0      0.759      0.241   \n",
       "\n",
       "   vader_compound  00 30  00 gmt      ...       york evidence support  \\\n",
       "0           0.000    0.0     0.0      ...                         0.0   \n",
       "1           0.743    0.0     0.0      ...                         0.0   \n",
       "\n",
       "   york evidence support theory  yosemite national  yosemite national park  \\\n",
       "0                           0.0                0.0                     0.0   \n",
       "1                           0.0                0.0                     0.0   \n",
       "\n",
       "   yosemite national park oc  youtube channel  youtube video  zelda cube  \\\n",
       "0                        0.0              0.0            0.0         0.0   \n",
       "1                        0.0              0.0            0.0         0.0   \n",
       "\n",
       "   zoe van  zoe van dijk  \n",
       "0      0.0           0.0  \n",
       "1      0.0           0.0  \n",
       "\n",
       "[2 rows x 7008 columns]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df.drop('Comments', axis = 1)\n",
    "y = df.Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.60394265  0.76481149  0.66965889  0.62118492  0.64452424  0.62298025\n",
      "  0.67266187  0.68884892  0.69064748  0.70323741]\n",
      "0.668249812034\n"
     ]
    }
   ],
   "source": [
    "cv = StratifiedKFold(n_splits=10, random_state=42)  \n",
    "dt = DecisionTreeClassifier(max_depth= 10)\n",
    "scores = cross_val_score(dt, X, y, cv=cv)\n",
    "print scores\n",
    "print np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.60573477  0.76481149  0.67145422  0.62298025  0.6481149   0.62118492\n",
      "  0.67086331  0.67985612  0.68705036  0.70503597]\n",
      "0.667708630339\n"
     ]
    }
   ],
   "source": [
    "cv = StratifiedKFold(n_splits=10, random_state=42)  \n",
    "rfc = RandomForestClassifier(n_estimators= 200)\n",
    "\n",
    "scores = cross_val_score(dt, X, y, cv=cv)\n",
    "print scores\n",
    "print np.mean(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70884594577190507"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad = GradientBoostingClassifier()\n",
    "cross_val_score(grad, X, y, cv=cv).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "With GradientBoostingClassifier we Improved our accuracy score from .66 to .70 which is approx. 20% more than the baseline."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
